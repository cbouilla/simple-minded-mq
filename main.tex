\documentclass[a4paper,UKenglish,cleveref, autoref]{lipics-v2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{kbordermatrix}
\usepackage{blkarray}
\usepackage{parskip}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage{framed}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}

\newcommand{\bits}{\left\{0, 1\right\}}
\newcommand{\bigO}[1]{\ensuremath{\mathcal{O}\left( #1 \right)} }
\newcommand{\bigOsoft}[1]{\ensuremath{\mathcal{\tilde O}\left( #1 \right)} }
\newcommand{\bigOmega}[1]{\ensuremath{\Omega\left( #1 \right)} }
\newcommand{\bigTheta}[1]{\ensuremath{\Theta\left( #1 \right)} }
\newcommand{\Prob}{\mathrm{Pr}}

\newcommand{\red}{\color{red}}
\newcommand{\FIXME}{{\red \textbf{FIXME}}\xspace}
\newcommand{\TODO}[1]{{\red \textbf{TODO}:} #1\xspace}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{A Simple Deterministic Algorithm for Systems of Quadratic Polynomials over $\mathbb{F}_2$}


\author{Charles Bouillaguet}{LIP6 laboratory, Sorbonne Université, Paris, France}{charles.bouillaguet@lip6.fr}{https://orcid.org/0000-0001-9416-6244}{} 
\author{Claire Delaplace}{MIS Laboratory, Université de Picardie Jules Verne, Amiens, France}{claire.delaplace@u-picardie.fr}{}{}
\author{Monika Trimoska}{MIS Laboratory, Université de Picardie Jules Verne, Amiens, France}{monika.trimoska@u-picardie.fr}{}{}

\authorrunning{C. Bouillaguet, C. Delaplace and M. Trimoska}

\Copyright{Charles Bouillaguet, Monika Trimoska and Claire Delaplace}

%\ccsdesc[100]{Theory of computation~Computational complexity and cryptography}
%\ccsdesc[100]{Theory of computation}% Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Boolean quadratic polynomials, exhaustive search, linear algebra}
\supplement{Source code available at : \url{https://gitlab.lip6.fr/almasty/moebius/}}
\funding{blabla ANR ASTRID PostCryptum}

\hideLIPIcs
\nolinenumbers

\begin{document}
\maketitle

\begin{abstract}
  This article discusses a simple deterministic algorithm for solving quadratic
  Boolean systems which is essentially a special case of more sophisticated
  methods. The main idea fits in a single sentence: guess enough of variables so
  that the remaining quadratic equations can be solved by linearization
  (\textit{i.e.} by considering each remaining monomial as an independent
  variable and solving the resulting linear system). Under strong heuristic
  assumptions, this finds all the solutions of $m$ quadratic polynomials in $n$
  variables with $\bigOsoft{2^{n-\sqrt{2m}}}$ operations. Although this the best
  known algorithm require exponentially less operations, the present technique
  has the advantage of being simpler to describe and easy to implement. In
  strong contrast with the state-of-the-art, it is also quite efficient in
  practice.
\end{abstract}


\clearpage

\section{Introduction}

We consider the problem of solving systems of multivariate quadratic boolean
polynomial equations. Given a set of quadratic boolean polynomials
$\{f_1, \dots, f_m\}$ in $\mathbb{F}_2[x_1, \dots, x_n]$, the problem consists
in finding a satisfying assignment $\hat x \in \bits^n$ such that
$f_i(\hat x) = 0$ for $1 \leq i \leq m$, or determining that none exist. Each
polynomial is represented as the sum of quadratic terms $x_i x_j$, linear terms
$x_i$ and a constant term. Because $x^2 = x$ modulo 2, we assume that all
monomials are square-free.


% A quadratic boolean
% polynomial can be represented by a constant $c$ and an $n \times n$ matrix $A$
% such that $f(x) = c + x A x^t$, which expands to
% $c + \sum_{i=1}^n \sum_{j=1}^n A[i,j] x_i x_j$ ; $c$ is the constant term, 

It is well-known that the problem is NP-complete, with a simple reduction from
SAT. It is relevant to the cryptology community because its hardness can be used
to build ``post-quantum'' encryption or signature schemes. Many have been
proposed with concrete sets of parameters, such as HFE~\cite{Patarin96},
UOV~\cite{KipnisPG99}, MQDSS~\cite{ChenHRSS16} and variants thereof. Several
have been submitted to the ongoing competition launched in 2017 by the
(American) National Institute of Standards and Technology in order to select a
portfolio of ``post-quantum'' public-key cryptographic algorithms. Among these,
the \textsf{GeMSS} signature scheme exposes a public key consisting of $m=162$
boolean quadratic polynomials in $n=192$ variables. It follows that the relevant
range of parameters for cryptographic instances is essentially $m \approx n$ and
$n \approx 200$ at this point:
% GeMMS: 162--324
solving quadratic boolean systems of this size is assumed to be intractable
(much smaller systems seem practically out of reach as well). Note that in the
context of cryptology, the average complexity is more relevant than the worst
case, because cryptographic instances of the problem are (presumably
indistinguishable from) random. In addition, there is empirical evidence that
random systems are hard to solve. The algorithmic problem is thus both of
theoretical and of practical interest, and many algorithms have been proposed to
solve it.

In this paper, we present a \emph{decremental} improvement over the
state-of-the-art: a simple deterministic algorithm that achieves a
sub-exponential advantage over exhaustive search in the average case, while
being extremely simple and easy to implement. Its complexity is inferior to
other existing algorithms, which are exponentially better, but its description
is much simpler and its practical efficiency is much better. The algorithm works
as follows:

\begin{framed}
  Guess sufficiently many variables so that the remaining polynomial system can
  be solved by \emph{linearization} (\textit{i.e.} by considering each remaining
  monomial as an independent variable, solving the resulting linear system and
  checking each solution against the original polynomial system).
\end{framed}

More precisely, guess the values of all variables except the
$\left\lfloor \sqrt{2m} - 2 \right\rfloor$ last ones. There remain strictly
less than $m$ (non-constant) monomials of degree less than two in the remaining
variables, which enables the use of the linearization technique.

This algorithm is a particularly simple special case of several other more
complex algorithms from the cryptology community, including but not limited
to~\cite{CourtoisKPS00,BettaleFP09,JouxV17}. However, to the best of our
knowledge, this simple form was not discussed \textit{per se}.

\subsection{Related Work}
\label{sec:related}

Exhaustive search is the baseline method to solve systems of boolean quadratic
polynomials, with a running time $\bigOsoft{2^n}$ and negligible space
complexity. Using several algorithmic tricks and low-level optimizations, it can
be implemented extremely efficiently: serious implementations check several
candidate solutions per CPU cycle~\cite{BouillaguetCCCNSY10}, which means that
the factors hidden in the big Oh notation are extremely small. It follows that
``beating brute force'' \emph{in practice}, namely assembling an implementation
that runs faster on existing hardware than exhaustive search, for problem sizes
that are feasible, is a significant achievement.

Systems that are very underdetermined ($n \geq m^2$) or very overdetermined
($m \geq 0.5 n^2$) can be solved in polynomial time by simple
techniques~\cite{CourtoisGMT02}. This suggest that $m \approx n$ is the hardest
case, and it is common in cryptology (we usually have $m=2n$ for encryption and
$n = 2m$ or $n=3m$ for signatures). The overdetermined case is relevant to us, as
it is the linearization technique: there are
$\binom{n}{2} + \binom{n}{1} + \binom{n}{0} = 1 + n(n+1)/2$ boolean monomials in
$n$ variables ; consider each one as an independent fresh variable ; provided
there are as many (linearly independent) quadratic equations, this yields a
linear system with a small number of solutions. This system can be solved in
polynomial time, and each solution reveals a possible value of the variable. On
random input systems, the number of solutions is expected to be only one.

The next family of algorithms are algebraic manipulation techniques that derive,
in a way or another, from the Buchberger algorithm for computing Groebner
bases. Given a Groebner basis of the original polynomial equations, it is easy
to read a potential solution. These algorithms are neither limited to quadratic
polynomials nor to the boolean field. Their average case complexity is
notoriously difficult to study, and it requires algebraic assumptions
(regularity or semi-regularity) on the input polynomials~\cite{BardetFS15}.  The
state of the art, at this point, seems to be the F4 and F5 algorithms by
Faugère~\cite{F4,F5}. F4 is essentially a reformulation of the Buchberger
algorithm that replaces polynomial manipulations by efficient sparse linear
algebra and processes them in batch. F5 strives to eliminate some useless
computation. In Bardet's thesis, it is shown that a simplified version of F5
computes a Groebner basis of a regular sequence of polynomial in
$\bigOsoft{2^{4.295n}}$ field operations, over any finite field. Efficient
implementations of F4 are available in off-the-shelf computer algebra systems,
notably \textsf{MAGMA}~\cite{MAGMA}. Faugere's algorithms have been successful
in breaking some cryptosystems, most notably an instance of HFE with $n=80$
variables, which turned to be spectacularly weak against Groebner basis
computations~\cite{FaugereJ03}. Variants of these algorithms have been
rediscovered by the crypto community under the name XL, notably by Courtois,
Klimov, Patarin and Shamir~\cite{CourtoisKPS00}.

All these algorithms have exponential space complexity and existing
implementation run into memory limitations even for a moderate number of
variables. Implementing them is difficult, because they require either
sophisticated data-structure for large-degree multivariate polynomials and/or
sparse linear algebra over large matrices. Existing implementations are usually
available inside full-blown computer algebra systems, which are large and
complex software projects.

It is well-known that solving systems by Groebner basis computation is easier on
overdetermined systems. In the extreme, on sufficiently overdetermined
polynomial systems, Groebner basis computations degenerate into simple linear
algebra and works in polynomial time. This is the basis for the ``hybrid
method'' which combines exhaustive search and algebraic techniques: guess the
values of some variables, then compute a Groebner basis of the remaining system
which has become overdetermined. Bettale, Faugère and Perret discuss the optimal
number of variables to fix~\cite{BettaleFP09}. The \textsf{BooleanSolve}
algorithm of Bardet, Faugère, Salvy and Spaenlehauer~\cite{BardetFSS13} is the
best at this point, with running time $\bigOsoft{2^{0.792n}}$ on average, under
algebraic assumptions. It guesses some variables, then checks if a polynomial
combination of the remaining polynomial is equal to 1. When it is the case, then
the guessed values are incorrect (by Hilbert's Nullstellensatz). This in turns
is accomplished by deciding whether large sparse linear systems have a
solution. Its inventors claim that it is slower than exhaustive search when
$n \leq 200$, which seems to make it practically useless. While conceptually
simple, the algorithm is likely difficult to implement because it requires a
sparse linear system solver for large matrices (and it requires exponential
space). It must be noted that the algorithm which is the center of this paper is
essentially a special case of the hybrid method.

The \textsf{Crossbred} algorithm of Joux and Vitse~\cite{JouxV17} also belongs
to this family ; its complexity is unknown, but its practical efficiency is
spectacular: it has been used to solve a random system with $n = 74$ variables
and $m=148$ equations (this would require about 150 million CPU hours using
exhaustive search). There is a public implementation that uses GPUs by
Niederhagen, Ning and Yang~\cite{NiederhagenNY18}. This algorithm is discussed
more in-depth in section~\ref{sec:extensions}.

A completely different family of algorithms emerged in 2017 when Lokshtanov,
Paturi, Tamaki, Williams and Yu~\cite{LokshtanovPTWY17} presented a randomized
algorithm of complexity $\bigOsoft{2^{0.8765n}}$. In strong contrast with almost
all the previous ones, the algorithm does not require any assumption on the
input polynomials, which is a theoretical breakthrough. The algorithm works by
assembling a high-degree polynomial that evaluates to 1 on partial solutions,
then approximates it by lower-degree polynomials. The technique was later
improved by Björklund, Kaski and Williams~\cite{BjorklundK019}, reaching
$\bigOsoft{2^{0.804n}}$, then again by Dinur~\cite{Dinur21}, reaching
$\bigOsoft{2^{0.6943n}}$.

Dinur proposed a lightweight, ``concretely efficient'' version of his algorithm
for the crypto community with complexity $\bigO{n^2 2^{0.815n}}$ using
$n^2 2^{0.63n}$ bits of memory. This is concretely impractical: it requires more
operation than exhaustive search as long as $n \leq 65$, and when it gets faster
(for $n \geq 66$) it requires at least 1.5 petabyte of memory. All other
incarnations of the ``polynomial
method''~\cite{LokshtanovPTWY17,BjorklundK019,Dinur21} are even worse from a
practical standpoint.  They are therefore mostly of theoretical interest.


\section{A Toy Example}

Consider the following system in 5 variables:
\begin{align*}
  f_1 &= ae + bc + be + cd + a + d + e + 1,\\
  f_2 &= ac + ad + ae + bc + bd + ce + de + a + b + d,\\
  f_3 &= ad + be + cd  + a + b + d + 1,\\
  f_4 &= ab + ad + bd + be + b + d + e,\\            
  f_5 &= ab + ae + bc + bd + cd + ce + de + a + e + 1
\end{align*}



These polynomials can be seen as vectors in the vector space spanned by all
monomials. The system can thus be written as a matrix:
\[
  M = \begin{blockarray}{ccccccccccccccccc}
  ab & ac & ad & ae & bc & bd & be & cd & ce & de & a & b & c & d & e & 1 & \\
  \begin{block}{[cccccccccccccccc]c}
     &    &    & 1  & 1  &    & 1  & 1  &    &    & 1 &   &   & 1 & 1 & 1 & f_1\\
     & 1  & 1  & 1  & 1  & 1  &    &    & 1  & 1  & 1 & 1 &   & 1 &   &   & f_2 \\
     &    & 1  &    &    &    & 1  & 1  &    &    & 1 & 1 &   & 1 &   & 1 & f_3 \\
   1 &    & 1  &    &    & 1  & 1  &    &    &    &   & 1 &   & 1 & 1 &   & f_4 \\
   1 &    &    & 1  & 1  & 1  &    & 1  & 1  & 1  & 1 &   &   &   & 1 & 1 & f_5 \\
    \end{block}
\end{blockarray}
\]

Next, separate the first $u=3$ variables, and write the polynomials in
$\mathbb{F}_2[a,b,c][d,e]$, \textit{i.e.} as polynomials in $d, e$ whose coefficients are themselves polynomials in $a, b, c$:
\begin{equation}
  \label{eq:example_matrix}
  M(a,b,c) = \begin{blockarray}{ccccc}
  de & d & e & 1 & \\
  \begin{block}{[c|c|c|c]c}
  0 & c+1 & a+b+1 & bc + a + 1 & f_1\\
  1 & a+b+1 & a+c     & ac +bc + a+b & f_2\\
  0 & a+c+1 & b & a+b+1          & f_3 \\
  0 & a+b+1 & b+1 & ab+b  & f_4 \\
  1 & b+c & a+c+1 & ab+bc+a+1 & f_5 \\
\end{block}
\end{blockarray}
\end{equation}
This yields a matrix with coefficients in $\mathbb{F}_2[a,b,c]$. More precisely,
the columns corresponding to quadratic (resp. linear, constant) monomials in
$d,e$ contain constant (resp. linear, quadratic) terms in $a,b,c$. % Any solution
% to the initial polynomial system is also a solution of the following linear system:
% \begin{equation}
%   \label{eq:example_system}
%   \underbrace{\begin{pmatrix}
%   0 & c+1   & a+b+1 \\
%   1 & a+b+1 & a+c   \\
%   0 & a+c+1 & b     \\
%   0 & a+b+1 & b+1   \\
%   1 & b+c   & a+c+1 \\
%     \end{pmatrix}}_{L(a,b,c)}
%   \begin{pmatrix}
%     de \\
%     d \\
%     e\\
%   \end{pmatrix}
%   =
%   \underbrace{\begin{pmatrix}
%   bc + a + 1 \\
%   ac +bc + a+b \\
%   a+b+1 \\
%   ab+b \\
%   ab+bc+a+1 \\
% \end{pmatrix}}_{Q(a,b,c)}
% \end{equation}

Perform linear combinations of the rows to put the columns corresponding to
quadratic terms in reduced row echelon form :
\[
  \widetilde M(a,b) = \begin{blockarray}{ccccc}
  de & d & e & 1 & \\
  \begin{block}{[c||c|c|c]c}
    1 & a+b+1 & a+c     & ac +bc+a+b & f_2\\
    \BAhline\BAhline
    0 & c+1 & a+b+1 & bc + a + 1 & f_1\\
    0 & a+c+1 & b & a+b+1          & f_3 \\
    0 & a+b+1 & b+1 & ab+b  & f_4 \\
    0 & a+c+1 & 1 & ab+ac +b+1 & f_2 + f_5 \\
  \end{block}
\end{blockarray}
\]


Any solution to the initial polynomial system is also a solution of the
following equations, taken by extracting non-pivotal rows:
\[
  \underbrace{\begin{pmatrix}
    c+1   & a+b+1 \\
    a+c+1 & b     \\
    a+b+1 & b+1   \\
    a+c+1 & 1     \\
  \end{pmatrix}}_{L(a,b,c)}
  \begin{pmatrix}
    d \\
    e\\
  \end{pmatrix}
  =
  \underbrace{\begin{pmatrix}
  a + bc + 1 \\       
  a + b + 1 \\  
  ab + b  \\             
  ab + ac + b + 1 \\
\end{pmatrix}}_{Q(a,b,c)}
\]

Enumerate all the possible values of the first three variables $(a, b, c)$ ; for
each combination, solve the linear system $L(a,b,c) \cdot (d,e)^t = Q(a,b,c)$
for $(d,e)$. Any solution of the linear system is automatically a satisfying
assignment for $\{f_1, f_3, f_4, f_2 + f_5\}$. Check candidate solutions against
$f_2$ ; they are then guaranteed to satisfy the original system.

The linear system, which is overdetermined, is inconsistent except for
$(a,b,c) = (1,0,1)$, where it admits a single solution $(e, d) = (0, 0)$. This
solution is indeed a valid satisfying assignment.

\section{Formal Description and Heuristic Analysis}
\label{sec:description}

% We use hats to denote concrete values. By default, vectors are row vectors. The
% transpose of a vector $x$ is the column vector $x^t$.

\TODO{ceci contient l'amélioration car ça simplifie la description. Monika: J'ai mis une proposition pour ce TODO, voir 2ème paragraphe. }

\begin{algorithm}[t]
  \caption{\label{the-algo}}
\begin{algorithmic}[1]
  \State Let $A$ denote a $\ell \times v$ matrix of bits and $b$ a size-$\ell$
  vector of bits
  \State Compute a basis $g_1, \dots, g_\ell$ of $\mathcal{V}$
  \State Write
  $g_i(y, z) = h_i(y) + y\cdot B_i \cdot z^t + c_i \cdot z^t + D_i$
  \For{$\hat y \in \bits^u$}

  \For{$1 \leq i \leq \ell$}
  \State $b[i] \gets  h_i(\hat y)$
  \State $A[i, \cdot] \gets \hat y \cdot B_i + c_i$
  \EndFor

  \State Solve the linear system $Az^t = b$

  \For{each solution $\hat z$}
  \If{$0 = f_1(\hat y, \hat z) = \dots = f_m(\hat y, \hat z)$}
  \State \Return $(\hat y, \hat z)$
  \EndIf
  \EndFor
  \EndFor
  \State \Return $\bot$
\end{algorithmic}
\end{algorithm}


Suppose that the $n$ variables $x = (x_1, \dots, x_n)$ are arbitrarily
partitioned in two sets with $x = (y, z)$, $y = (y_1, \dots, y_{u})$,
$z = (z_1, \dots, z_{v})$ and $u + v = n$. In the sequel, we choose
$v = \left\lfloor \sqrt{2m} - 2 \right\rfloor$. This choice guarantees that
there are less than $m$ non-constant quadratic monomials in the variables
$z_1, \dots, z_v$ and will make it possible to solve quadratic systems of $m$
equations in $v$ variables by linearization. Indeed, there are $v(v+1)/2$ such
monomials, and this evaluates to $m - 3v/2 - 2$ without rounding $v$ towards
zero.

The second step of our simple algorithm consists in solving the linearized system of equations. The most straightforward method is to perform a Gaussian elimination on the entire system. Since Gaussian elimination has a significant computational cost and our choice of $v$ ensures that system is overdetermined, it can be beneficial to consider only a subset of the system, find its solution and then, check whether the solution is consistent with the original system. Choosing the size and composition of this subset introduces a trade-off between the cost of the Gaussian elimination and the number of solutions to check. We present here a choice that both allows for a simple formal description, and is standard in the literature~\cite{--,JouxV17}. \TODO{rajouter citation d'autres algos qui utilisent cette méthode}

Consider the linear space spanned by $f_1, \dots, f_m$, and let $\mathcal{V}$
denote its subspace where all the coefficients of all quadratic monomials
$z_i z_j$ are equal to zero. Computing a basis $g_1, \dots, g_\ell$ of
$\mathcal{V}$ is easy (this can be done by putting the original polynomials in
reduced row echelon form). The point is that once the values $y_1, \dots, y_u$
are fixed, then the $g_i$ polynomials become linear and only depend on
$z_1, \dots, z_v$. The algorithm works by enumerating all possible $\hat y \in \bits^u$, solving
$g_1(\hat y, z) = \dots = g_\ell(\hat y, z) = 0$, then checking each candidate
solution $(\hat y, \hat z)$ against the original system. This is shown in algorithm~\ref{the-algo}.

We write each $g_i$ as $g_i(y, z) = h_i(y) + y \cdot B_i z^t + c_i z^t$, where
$h$ is a quadratic polynomial in $\mathbb{F}_2[y]$, $B_i$ is a $u \times v$
matrix and $c_i$ is a length-$v$ vector (this decomposition always exist).

Assuming that the input polynomials are linearly independent (which seems a mild
assumption), then the subspace $\mathcal{V}$ has dimension
$\ell = m - v(v-1)/2$. Our choice of $v$ guarantees that $\ell \geq 5v/2$.

Assembling the linear system (steps 5--7) requires evaluating the $\ell$
quadratic polynomials $h_i$'s in $u$ variables and performing $\ell$
matrix-vector products with the $B_i$'s which are of size $u \times v$.  This
requires $\bigO{\ell u(u+v)}$ operations, while solving the linear system using
Gaussian elimination requires $\bigO{\ell v^2}$ operations.

Let us assume that $m = \bigTheta{n}$, so that $v = \bigO{\sqrt{n}}$ and
$\ell = \bigO{\sqrt{n}}$. Assembling the linear system costs $\bigO{n^{2.5}}$
while solving it costs $\bigO{n^{1.5}}$. The cost of assembling the linear
system is dominated by the evaluation of the quadratic polynomials. It is
possible to decrease this cost to $\bigO{n}$, as described in
section~\ref{sec:extensions}.

The main problem of this algorithm is that it is difficult to bound the total
number of iterations of the solution-checking loop (step 9--11). Morally
speaking, because the linear system $A z^t = b$ is quite overdetermined, then most
of the time there should be no solution at all.

Let us make the heuristic assumption that the vectors $b$ are uniformly random
(in fact, they are the result of the evaluation of quadratic polynomials). The
image of $A$ is a subspace of dimension less than or equal to $v$ in
$\bits^\ell$, therefore it contains the random vector $b$ with probability less
than $2^{v-\ell}$. When the linear system is consistent, it has at most $2^v$
solutions. This yields a crude upper-bound on the expected number of solutions
$N$ of each random linear system:
\[
  E(N) \leq 2^v \mathrm{Pr}(\text{the linear system is consistent}) = 2^{2v - \ell} \leq 2^{-v/2}
\]
It follows that the total time spent checking solutions (in steps 9--11) is
asymptotically negligible compared to the rest of the algorithm.  The total
expected running time of the algorithm, under the heuristic assumption that the
$b$ vectors are random, is $\bigO{n^{2.5} 2^{n - \sqrt{2m}}}$.

\TODO{semi-regularité permet de se débarasser de l'hypothèse ?}

\section{Potential Extensions}
\label{sec:extensions}

This section discusses

\subsection{Faster Polynomial Enumeration Using a Gray Code}

As discussed in section~\ref{sec:description}, the running time of the algorithm
is dominated by the need to evaluate quadratic polynomials (step 7 of
algorithm~\ref{the-algo}). Evaluating a quadratic polynomial on
$\hat y \in \bits^u$ naively requires $\bigO{u^2}$ operations. This can be
reduced by enumerating all values of $\hat y$ using a Gray code, so that only a
single bit of $\hat y$ changes at each iteration. This technique seems to belong
to the folklore. The point is that once the value of $q_i(\hat y)$ is known,
only the monomials that depend on the flipped variable must be reevaluated, and
there are only $u$.

Consider an arbitrary quadratic polynomial
\[
  f(x) = c + \sum_{i=1}^u \sum_{j=1}^u M[i,j] x_i x_j.
\]
Observe that $f(x + y) = f(x) + f(y) + x \left(M + M^t\right)y^t + f(0)$. Let
$\hat e_k \in \bits^u$ denote the vector which is zero everywhere except on the
$k$-th coordinate.  Define the partial ``derivative'' of $f$ with respect to its
$k$-th variable as:
\[
  \frac{\partial f}{\partial k}(y) = f(y) + f(y + \hat e_k).
\]
Using the above observation, one quickly find that:
\[
  \frac{\partial f}{\partial k}(y) = M[k,k] + \sum_{i=1}^u (M[i,k] + M[k, i]) y_i.
\]

This suggests the following rearrangement of algorithm~\ref{the-algo}:

\begin{algorithmic}[1]
  \State $\hat y \gets 0$ \Comment{Setup $\hat y$, $A$ and $b$}
  \For{$1 \leq i \leq \ell$}
  \State $b[i] \gets  h_i(0)$
  \State $A[i, \cdot] \gets c_i$
  \EndFor
  \For{$1 \leq i \leq 2^u$} \Comment{Main loop}
  \State Solve the linear system $Az^t = b$ and process its solution as before
  \State $k \gets $ index of the rightmost bit of $i$ \Comment{Update $\hat y$}
  \State $\hat y \gets \hat y + \hat e_k$
  \For{$1 \leq i \leq \ell$} \Comment{Update $A$ and $b$}
  \State $b[i] \gets b[i] + \frac{\partial q_i}{\partial k}(\hat y)$
  \State $A[i, \cdot] \gets A[i, \cdot] + B_i[k, \cdot]$
  \EndFor
  \EndFor
\end{algorithmic}

Maintaining $A$ and $b$ consistent with the single-bit updates to $\hat y$ now
requires $\bigO{u}$ operations on step 10 and $\bigO{v}$ operations on step
11. All in-all, the total cost of assembling the linear system has dropped from
$\bigO{n^{2.5}}$ to $\bigO{n^{1.5}}$, and it now matches that of solving it.

Lastly, this could be again improved a little by observing that each individual
``partial derivative'' is evaluated on related inputs (only two bits differ from
one evaluation to the next). Taking advantage of this observation leads to the
fast exhaustive search algorithm of Bouillaguet, Chen, Cheng, Chou, Niederhagen,
Shamir and Yang~\cite{BouillaguetCCCNSY10}. Using this technique allows to
update each $b[i]$ in constant time, and brings down the total time needed to
update $A$ and $b$ to $\bigO{n}$.

\subsection{Using Higher-Degree Multiples}

\TODO{Monika : Discuter ici qu'on peut aller vers crossbred complet}

L'idée simple de départ peut se généraliser de DEUX façon différentes

1) Joux-Vitse [Claire !]

\TODO{à adapter}
Consider the linear space spanned by $f_1, \dots, f_m$, and let $\mathcal{V}$
denote its subspace where all the coefficients of all quadratic monomials
$z_i z_j$ are equal to zero. Computing a basis $g_1, \dots, g_\ell$ of
$\mathcal{V}$ is easy (this can be done by putting the original polynomials in
reduced row echelon form). The point is that once the values $y_1, \dots, y_u$
are fixed, then the $g_i$ polynomials become linear and only depend on
$z_1, \dots, z_v$.


Let $\mathcal{V}_d$ denote the subspace of $\mathbb{F}_2[x]$ spanned by
$m_i \cdot f_j$ where $m_i$ ranges across all monomials of degree $d-2$ and
$1 \leq j \leq m$. Consider its intersection with the subspace where all
monomials of total degree 2 or more in the $z_i$. Et bailable.


% sage: # vrai JV                                                                                                             
% sage: eqs = (n+1) * m                                                                                                         
% sage: # vrai JV (en degré 3)                                                                                                  
% sage: a_eliminer = binomial(v, 3) + binomial(v, 2) * (n-v+1)                                                                  
% sage: solve(eqs == a_eliminer + v, v)                                                                                         
% [v == -1/4*(1/3)^(2/3)*(3*(n + 1)^2 - 6*n + 10)*(-I*sqrt(3) + 1)/(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3) - 1/4*(1/3)^(1/3)*(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3)*(I*sqrt(3) + 1) + 1/2*n + 1/2, v == -1/4*(1/3)^(2/3)*(3*(n + 1)^2 - 6*n + 10)*(I*sqrt(3) + 1)/(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3) - 1/4*(1/3)^(1/3)*(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3)*(-I*sqrt(3) + 1) + 1/2*n + 1/2, v == 1/2*(1/3)^(2/3)*(3*(n + 1)^2 - 6*n + 10)/(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3) + 1/2*n + 1/2*(1/3)^(1/3)*(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3) + 1/2]
% sage: x = -1/4*(1/3)^(2/3)*(3*(n + 1)^2 - 6*n + 10)*(-I*sqrt(3) + 1)/(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(
% ....: -27*(8*m + 1)*n^4 - 108*(2*m - 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1
% ....: 225/3))^(1/3) - 1/4*(1/3)^(1/3)*(3*(n + 1)^3 - 36*m*(n + 1) - 3*(3*n - 5)*(n + 1) + sqrt(-27*(8*m + 1)*n^4 - 108*(2*m - 
% ....: 1)*n^3 + 6*(216*m^2 - 180*m - 47)*n^2 + 1296*m^2 + 108*(24*m^2 - 22*m + 5)*n - 1296*m - 1225/3))^(1/3)*(I*sqrt(3) + 1) +
% ....:  1/2*n + 1/2

Dire en degré 3, limite $v$ tends vers $\sqrt{2m}$.

2) linéarisation simple [NEW ! Monika !]
Générer des multiples
Éliminer tous les monômes de degré D
On obtient un système de degré D-1, soluble par linéarisation

Discussion aspect pratique pas mieux sauf si $m$ très grand.


Dire en degré grand c'est chiant à cause du degré de régularité. Série de Hilbert et blablabla. Hypothèse de (semi-régularité) $\leadsto$ calcul des dimensions de l'espace des multiples en grand degré.



\section{Practicality}

This algorithm can actually be implemented and executed. Toy implementations in
a computer algebra systems such as \textsf{SageMath} are easy to write. A
user-friendly and competitive implementation in pure C using low-level
optimizations is about 650 lines long (it is accessible in the supplementary
material). This is possible because the algorithm itself is simple, and because
it does not rely on sophisticated data structures or complex sub-algorithms such
as fast multivariate polynomial multiplication, fast multipoint
evaluation/interpolation, groebner basis computations or large sparse linear
system solvers. In addition, its space complexity is negligible and it is
trivially parallelizable.

We discuss an interesting implementation aspect that leads to a speedup of about
$40 \times$ on contemporary hardware, and makes the implementation
competitive. The program spends much of its time examining a large number of
very small overdetermined linear systems modulo 2 (say, $20 \times 10$). The
overwhelming majority will be full-rank and inconsistent, therefore detecting
that one of these conditions is not met is almost sufficient. But the actual key
to obtain good software performance is \emph{vectorization}.

Consider a Boolean circuit $\mathcal{C}$ with $(\ell+1)v$ input wires for $A$
and $b$ as well as $v$ output wires for $z$ and two extra output wires $c$ and
$d$. The $c$ output wire indicates whether the linear system $A z^t = b$ is
\emph{consistent} while the $d$ output wire indicates whether the matrix $A$ is
\emph{rank-defective}. We use the following relaxed specification: if $d=1$ (the
system does not have full-rank), then the other outputs are unspecified. If
$d=0$ and $c=0$, then the system is inconsistent ; lastly, if $d=0$ and $c=1$
then the only solution of the system can be read on the $z$ wires.

The point is that on a CPU equipped with $w$-bit registers, $w$ copies of the
circuit can be evaluated in parallel on $w$ distinct inputs by performing normal
Boolean operations between registers (the $i$-th copy operates on the $i$-th bit
of all $w$-bit values). Most current \textsf{x86-64} CPUs have \textsf{AVX2}
instructions, which allows to perform Boolean operations on 256-bit registers.

This allows to group the iterations of the main loop of the algorithm in batches
of size $w$. The most common situation is that all linear systems are full-rank
and inconsistent (which results in $d \vee c = 0000 \dots 000$). In this case,
there is nothing to do except moving on to the next batch. Otherwise, the
individual elements of the batch must be examined.  

A circuit meeting this specification is not difficult to obtain (this amounts to
perform an LU factorization of $A$ with partial pivoting). Giving up on
producing a meaningful result when the system is rank-defective allows several
simplifications that reduce the size of the circuit. Actually obtaining the
solution $z$ when the system is consistent has negligible cost. Our circuit has
approximately $3\ell v^2$ gates.

Under heuristic randomness assumptions, each linear system is consistent with
probability $2^{v-\ell}$. We now crudely lower-bound the probability that a
random $\l \times v$ matrix is full rank. This happens when each column is
chosen out of the linear span of the previous columns, and the probability of
this event is:
\[
  p := \prod_{j=\ell-v+1}^\ell \left(1 - 2^{-j} \right)
\]
This is always greater than $\left(1 - 2^{v-ell} \right)^v$. Let $e$ denote the
\emph{excess ratio} $e := \ell/v - 1$. Because we always have $\ell \geq 2v$,
then $e \geq 1$. This implies that $p \geq \left(1 - 2^{-ev} \right)^v$. Taking
an asymptotic expansion for $v \rightarrow +\infty$ shows that this is
$1 - v 2^{-ev} + \bigO{v^2 2^{-2ev}}$. Therefore, we expect the proportion of
rank-defective linear systems to be smaller than $v 2^{v-\ell}$.


\subsection{Comparison with exhaustive search}

The wall-clock running time of fast software implementations of exhaustive
search is $T = \alpha 2^n$, for some constant $\alpha$ which depends on the
implementation and on the machine. The running time of this algorithm presented
in this paper is $T' = \beta P(m) 2^{n - 2\sqrt{m}}$, for some polynomial $P$
and some constant $\beta$ that also depends on the machine. An implementation of
our algorithm ``beats brute force'' when $T/T' > 1$, in other terms when
$\alpha 2^{\sqrt{2m}} > \beta P(m)$. This always happens for sufficiently large
$m$, \textit{i.e.} if there are sufficiently many equations.

Determining the threshold $m$ such that both algorithms take the same time is a
simple matter ; on the recent laptop of one of the authors, we found that it is
$m=48$, which we consider to be rather low.

Solving a random system of 512 quadratic equations in 64 variables takes less
than 15 minutes on a laptop using this algorithm ; it would take 16 years using
exhaustive search.

\bibliography{biblio}

\end{document}

%%% Local Variables:
%%% ispell-local-dictionary: "english"
%%% eval: (flyspell-mode 1)
%%% eval: (reftex-mode 1)
%%% End: